            +----------------------+
            |        OS 211        |
            |  TASK 1: SCHEDULING  |
            |    DESIGN DOCUMENT   |
            +----------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Alessandro Bonardi ab9515@ic.ac.uk
Andy Hume ah2814@ic.ac.uk
Tanmay Khanna tk915@ic.ac.uk
Thomas Bower tb1215@ic.ac.uk

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, or notes for the
>> markers, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> A1: (5 marks)
>> Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In thread.h, added following members to 'struct thread':

  int effective_priority;       - Effective Priority of the thread.
  struct list locks_held;       - List of currently held locks.
  struct lock *lock_to_acquire; - Lock currently acquired by another thread.

In synch.h, added the following member to 'struct lock':

  struct list_elem lock_elem;   - Shared with locks_held in struct thread.

>> A2: (10 marks)
>> Explain the data structure used to track priority donation.
>> Give a diagram that illustrates a nested donation in your structure.

Every thread holds a list of currently held locks (locks_held) inside the struct
and that is the main structure used to track priority donation. Whenever a
thread t1 acquires a lock, the lock gets added to the list; when a new thread t2
tries to acquire a lock currently held by thread t1, it is added to the list of
waiters inside the semaphore struct in the lock and the function
thread_update_effective_priority gets called on the holder.
This function returns the maximum value of the base priority and the donated
priority using the locks_held list: the effective priorities of each head of the
waiters in each lock of the list are compared to find a temporary maximum
priority, and that one is then compared to the base priority and assigned to the
actual effective priority.

Suppose we have three threads t1, t2 and t3 with base priorities 30, 35 and 40
respectively, as well as two locks l1 (owned by t1) and l2 (owned by t2). To
illustrate nested priority donation, let's say that t2 tries to acquire l1
and t3 tries to acquire l2, in that order, as explained by our diagram below.
Initially, our situation is as follows:

t1: Effective priority = 30
t2: Effective priority = 35
t3: Effective priority = 40

When t2 calls lock_acquire() on l1, it's lock_to_acquire member is set to l1
and then we down the semaphore within in the lock. Because the lock is owned by
t1, the semaphore value is 0, thus it is put onto the waiters list. Next we
have to donate the priority of t2 to the holder of l1, so
thread_update_effective_priority() is called on t1. After the effective priority
of t1 is updated, we check if t1 has a lock it wants to acquire (for nested
donation). As it doesn't, the function ends and we have the following situation.

t1: Effective priority = 35
t2: Effective priority = 35
t3: Effective priority = 40

The same process happens when we call lock_acquire() on l2 from t3 and we donate
t3's priority to t2. The situation reaches this point before we check the
lock_to_acquire member of t2:

t1: Effective priority = 35
t2: Effective priority = 40
t3: Effective priority = 40

However, we know that t2's lock_to_acquire member is set to l1. This means that
we also have to donate the priority to t1. So we call the
thread_update_effective_priority() function recursively on t1, which carries
the donation from t3 across to t1. This places us in the final state:

t1: Effective priority = 40
t2: Effective priority = 40
t3: Effective priority = 40

====================================DIAGRAM=====================================

   ╔═══════════════╗      ╔═══════════════╗     ╔═══════════════╗
   ║┌─────────────┐║      ║┌─────────────┐║     ║┌─────────────┐║
   ║│             │║      ║│             │║     ║│             │║
   ║│     t1      │║      ║│     t2      │║     ║│     t3      │║
┌─▶║│ priority 30 │║   ┌─▶║│ priority 35 │║     ║│ priority 40 │║
│  ║│             │║   │  ║│             │║     ║│             │║
│  ║│             │║   │  ║│             │║     ║│             │║
│  ║└─────────────┘║   │  ║└─────────────┘║     ║└─────────────┘║
│  ╚═══════════════╝   │  ╚═══════════════╝     ╚═══════════════╝
│                      │          │                     │
│       acquire() ─────┼──────────┘                     │
│          │           │       acquire()────────────────┘
│          │           │          │
│          │(1)        │          │(2)
│          │           │          │
│          ▼           │          ▼
│       ▬▬▬▬▬▬         │       ▬▬▬▬▬▬
│      ▮      ▮        │      ▮      ▮
│      ▮      ▮        │      ▮      ▮
│   ┌────────────┐     │   ┌────────────┐
│   │            │     │   │            │
│   │  LOCK L1   │     │   │  LOCK L2   │
│   │            │     │   │            │
└───│   HOLDER   │     └───│   HOLDER   │
    │            │         │            │
    │            │         │            │
    │            │         │            │
    └────────────┘         └────────────┘

---- ALGORITHMS ----

>> A3: (5 marks)
>> How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

For semaphores, we leave the waiters list unordered so that a thread's priority
changing (when it receives a donation) does not require re-sorting the waiters
list of a semaphore it is waiting to down. Instead, when a semaphore gets upped,
the element of the waiters list with the highest priority is awoken by searching
the list for this element.

This also covers the case of locks, as locks use a semaphore internally to
handle waiting threads.

However, condition variables have one semaphore per thread waiting, so to cover
this we use essentialy the same method as for semaphores: inserting the
semaphore_elems into the list unordered when a thread waits on a conditon, and
when the condition is signalled, searching the waiters list for the thread with
the highest priority by extracting the struct thread from the semaphore's wait
list.

>> A4: (5 marks)
>> Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When a thread calls lock_aquire(), if the lock is available the lock is added
to the locks_held list, otherwise the thread is added to the front of the
waiters list of the lock and priority donation is triggered in sema_down by
calling thread_update_effective_priority(). This function, as explained in A2,
compares the head of each waiters list in each lock; this list is "effectively"
ordered: when checking for the highest effective_priority it is certain that the
thread at the front has the highest priority among all the elements of the list,
as otherwise the thread which holds the lock would have run instead because of
priority donation from another thread in the list. If the donee thread is
blocked, i.e. it is a waiter, a recursive call is made on the holder of the lock
it is waiting for. At the same time, it is removed and re-inserted ordered from
the waiters list; otherwise, if it is a thread in the ready_list, it is removed
and re-inserted ordered from the ready_list.

>> A5: (5 marks)
>> Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

When lock_release() is called, the lock is first removed from the locks_held
list of the holder and thread_update_effective_priority() is called:
Now we compute the new effective_priority by finding the lock we are holding
with the highest priority waiter (found from the head of the locks wait list
-- this is the highest priority waiter because it is the thread that has most
recently run, which means it has the highest priority).
This eliminates the need for any recursive calls to update the effective
priority.
The function sema_up() is then called and the waiting thread with the highest
priority is awoken.

---- SYNCHRONIZATION ----

>> A6: (5 marks)
>> Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

A potential race in thread_set_priority() would happen when two threads want to
update their base priority at the same time. Unfortunately, we could not use a
lock to avoid this case, but only disabling the interrupts: when the base
priority changes, the function yield_if_higher_priority_ready() is called with
interrupts disabled as it accesses the priority resources using
get_thread_priority(), called from places where interrupts are disabled. A lock
could therefore not be used as locks can't be acquired in an interrupt context.

---- RATIONALE ----

>> A7: (5 marks)
>> Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Before using this design we were using a list of donors inside every thread
instead of a list of held locks. This donors list was ordered, so that the
highest priority could be donated in O(1) time. We then decided to use a list of
locks as we found the first solution to be a duplication data already present in
the waiters list. The waiters list itself is automatically ordered, and the only
ordered insertion happens when a change in the priority of one of the elements
occurs. At first we also had calls sto list_sort() but we replaced them at a
second time with list_remove() and list_insert_ordered(), preferring O(n) to a
theoretical O(n log n).

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> B1: (5 marks)
>> Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In newly added fixed-point.h, added the following typedef declaration:

  typedef int32_t fixed_point; - Type to represent a 17.14 fixed-point number.

In thread.h, added following members to 'struct thread':

  int nice;               - Nice value for the thread.
  fixed_point recent_cpu; - Estimate of CPU time used recently by the thread.

In thread.c, added following variables:

  static int ready_threads;                     - The number of threads ready
                                                  to run, cached to avoid
                                                  traversing the ready list
                                                  per load average computation.
  static fixed_point load_avg;                  - Estimates number of threads
                                                  to have run over the last
                                                  minute.
  static const fixed_point load_avg_factor;     - Constant factor (59/60) used
                                                  in load average calculation.
  static const fixed_point ready_thread_factor; - Constant factor (1/60) used
                                                  in load average calculation.
---- ALGORITHMS ----

>> B2: (5 marks)
>> Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59     A
 4      4   0   0  62  61  59     A
 8      8   0   0  61  61  59     A
12     12   0   0  60  61  59     B
16     12   4   0  60  60  59     A
20     16   4   0  59  60  59     B
24     16   8   0  59  59  59     A
28     20   8   0  58  59  59     B
32     20  12   0  58  58  59     C
36     20  12   4  58  58  58     A

>> B3: (5 marks)
>> Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behaviour of your scheduler?

Suppose we have a situation where the next thread ready to run has the same
priority as the currently running thread. The specification does not specify
whether the running thread should yield in this case.
  We resolve this ambiguity by the following rule. The running thread must
yield immediately if and only if the priority of the next ready thread is
strictly greater than the running thread's priority. This matches the behaviour
of our scheduler, as shown in our yield_if_higher_priority_ready() function.

>> B4: (5 marks)
>> How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

We update the BSD variables inside of thread_tick, which runs in an interrupt
context. This is undesirable, as we want to minimise the ammount of time we
spend inside interrupt handlers to improve responsiveness of the system and
avoid missing interrupts.
  We do take some steps to minimise this, such as keeping track of the number of
threads that are eligible to run in the static int ready_threads in thread.c,
which avoids traversing every element of the ready list in the interrupt
handler.
  Minimising the time spent in interrupt handlers gives better responsiveness
for interactive applications and keeps I/O devices busy.
  However, if we delegate the updating of the bsd variables to happen outside
of the interrupt handler, we will leave the ready list in an inconsistent state
(some threads will not have their priority updated based on up to date BSD
variables).

---- RATIONALE ----

>> B5: (5 marks)
>> Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.

Our implementation takes some steps to avoid doing unnecessary work.
  We only recalculate the priority for threads that are ready (rather than all
threads, as having fresh priority values for blocked threads is not necessary,
instead we recalculate values for blocked threads when they become unblocked.)
  Our use of macros to do fixed point arithmetic also eliminates a lot of
runtime overhead compared to if we used function calls.
  We also kept a number of threads that are ready to run (including the current
running thread, if it is not the idle thread). This is to avoid counting the
number of ready threads when we update load_avg in the interrupt handler.
  However, our implementation does a lot of the computation inside of an
interrupt context, which is a disadvantage for the reasons listed above.
  We considered implementing a lock on the ready list to get around this,
however we cannot acquire a lock from an interrupt context in thread_tick, and
if we updated the BSD variables in a separate thread which acquired the lock
outside of the interrupt, it may not run for a while and the BSD varaibles will
have outdated values.

>> B6: (5 marks)
>> The assignment explains arithmetic for fixed-point mathematics in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point mathematics, that is, an abstract
>> data type and/or a set of functions or macros to manipulate
>> fixed-point numbers, why did you do so?  If not, why not?S

For the fixed-point arithmetic we chose to create a new file in the threads
directory called fixed-point.h. In this file, we defined the 'fixed_point' type
to be an integer that represents a 17.14 fixed-point number, as well as the
fixed-point operations required by our scheduler.
  Due to the fact that we have to continuously update important fixed-point
values in the scheduler to calculate thread priorities, we chose to implement
these arithmetic operations in the form of macros rather than functions. Macros
are replaced by the pre-processor with the defined values, which meant there
was no unnecessary overhead at runtime from repeated calls.
