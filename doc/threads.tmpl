            +----------------------+
            |        OS 211        |
            |  TASK 1: SCHEDULING  |
            |    DESIGN DOCUMENT   |
            +----------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Alessandro Bonardi ab9515@ic.ac.uk
Andy Hume ah2814@ic.ac.uk
Tanmay Khanna tk915@ic.ac.uk
Thomas Bower tb1215@ic.ac.uk

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, or notes for the
>> markers, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> A1: (5 marks)
>> Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In thread.h, added following members to 'struct thread':

  int effective_priority;       - Effective Priority of the thread.
  struct list locks_held;       - List of currently held locks.
  struct lock *lock_to_acquire; - Lock currently acquired by another thread.

In synch.h, added the following member to 'struct lock':

  struct list_elem lock_elem;   - Shared with locks_held in struct thread.

In sync.c, added the following member to 'struct semaphore_elem':

  struct thread* t              - Keeps track of the thead waiting for that
                                  semaphore, to calculate the highest priority
                                  waiting thread when the condition is signalled

>> A2: (10 marks)
>> Explain the data structure used to track priority donation.
>> Give a diagram that illustrates a nested donation in your structure.
Every thread holds a list of currently held locks locks_held inside the struct
and that is the main structure used to track priority donation. Whenever a
thread t1 acquires a lock, the lock gets added to the list; when a new thread t2
tries to acquire a lock currently held by thread t1, it is added to the list of
waiters inside the semaphore struct in the lock and the function
thread_update_effective_priority gets called. This function returns the maximum
value of the base priority and the donated priority using the locks_held list:
the effective priorities of each head of the waiters in each lock of the list
are compared to find a temporary maximum priority, and that one is then compared
to the base priority and assigned to the actual effective priority.

In case of nested priority - where thread t2 holds a lock l2 and tries to
acquire a lock l1 currently held by t1, thread t3 is waiting for l2 and donates
its priority to t2 - t2 wants to donate the updated effective priority to t1. To
deal with this case, if the thread t2 is waiter, a recursive call of the function
thread_update_effective_priority() is made on the holder t1 of the lock
thread->lock_to_acquire, which is the lock that the thread t1 wanted to acquire but
was blocked as it was currently held by t2. [IMAGE]

---- ALGORITHMS ----

>> A3: (5 marks)
>> How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?



The ready_list list is an ordered list, ordered descending basing on the
effective_priority. This means that the thread returned by next_thread_to_run is
the head of that list.

>> A4: (5 marks)
>> Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?
When a thread calls lock_aquire(), if the lock is available the lock is added to the
locks_held list, otherwise the thread is added to the front of the waiters list of the
lock and priority donation is triggered in sema_down by calling
thread_update_effective_priority(). This function, as explained in A2, compares
the head of each waiters list in each lock; this list is "effectively" ordered:
when checking for the
highest effective_priority it is certain that the thread at the front has the
highest priority among all the elements of the list, as otherwise the thread
which holds the lock would have run instead because of priority donation from
another thread in the list. If the donee thread is blocked, i.e. it is a waiter, a
recursive call is made on the holder of the lock it is waiting for. At the same
time, it is removed and re-inserted ordered from the waiters list; otherwise, if it
is a thread in the ready_list, it is removed and re-inserted ordered from the
ready_list.

>> A5: (5 marks)
>> Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.
When lock_release() is called, the lock is firstly removed from the locks_held
list and thread_update_effective_priority is called: now the iteration is
carried over the updated locks_held list, and the new value of effective_priority
is computed and [THE RIGHT WORD CAN'T COME TO MIND!] in case of nested donation.
The function sema_up is then called and the waiting thread with the highest
priority is removed from the waiters list and added ordered to the ready_list.

---- SYNCHRONIZATION ----

>> A6: (5 marks)
>> Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?
A potential race in thread_set_priority() would happen when two threads want to
update their base priority at the same time. Unfortunately, we could not use a
lock to avoid this case, but only disabling the interrupts: when the base
priority changes, the function yield_if_higher_priority_ready() is called with
interrupts disabled as it accesses the priority resources using
get_thread_priority(), called from places where interrupts are disabled. A lock
could therefore not be used as locks can't be acquired in an interrupt context.

---- RATIONALE ----

>> A7: (5 marks)
>> Why did you choose this design?  In what ways is it superior to
>> another design you considered?
Before using this design we were using a list of donors inside every thread
instead of a list of held locks. This donors list was ordered, so that the
highest priority could be donated in O(1) time. We then decided to use a list of
locks as we found the first solution to be a duplication data already present in
the waiters list. The waiters list itself is automatically ordered, and the only
ordered insertion happens when a change in the priority of one of the elements
occurs. At first we also had calls sto list_sort() but we replaced them at a
second time with list_remove() and list_insert_ordered(), preferring O(n) to a
theoretical O(n log n).

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> B1: (5 marks)
>> Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In newly added fixed-point.h, added the following typedef declaration:

  typedef int32_t fixed_point; - Type to represent a 17.14 fixed-point number.

In thread.h, added following members to 'struct thread':

  int nice;               - Nice value for the thread.
  fixed_point recent_cpu; - Estimate of CPU time used recently by the thread.

In thread.c, added following variables:

  static int ready_threads;                     - The number of threads ready
                                                  to run, cached to avoid
                                                  traversing the ready list
                                                  per load average computation.
  static fixed_point load_avg;                  - Estimates number of threads
                                                  to have run over the last
                                                  minute.
  static const fixed_point load_avg_factor;     - Constant factor (59/60) used
                                                  in load average calculation.
  static const fixed_point ready_thread_factor; - Constant factor (1/60) used
                                                  in load average calculation.
---- ALGORITHMS ----

>> B2: (5 marks)
>> Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59     A
 4      4   0   0  62  61  59     A
 8      8   0   0  61  61  59     A
12     12   0   0  60  61  59     B
16     12   4   0  60  60  59     A
20     16   4   0  59  60  59     B
24     16   8   0  59  59  59     A
28     20   8   0  58  59  59     B
32     20  12   0  58  58  59     C
36     20  12   4  58  58  58     A

>> B3: (5 marks)
>> Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behaviour of your scheduler?

Suppose we have a situation where the next thread ready to run has the same
priority as the currently running thread. The specification does not specify
whether the running thread should yield in this case.
  We resolve this ambiguity by the following rule. The running thread must
yield immediately if and only if the priority of the next ready thread is
strictly greater than the running thread's priority. This matches the behaviour
of our scheduler, as shown in our yield_if_higher_priority_ready() function.

>> B4: (5 marks)
>> How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

We update the BSD variables inside of thread_tick, which runs in an interrupt
context. This is undesirable, as we want to minimise the ammount of time we
spend inside interrupt handlers to improve responsiveness of the system and
avoid missing interrupts.
  We do take some steps to minimise this, such as keeping track of the number of
threads that are eligible to run in the static int ready_threads in thread.c,
which avoids traversing every element of the ready list in the interrupt
handler.
  Minimising the time spent in interrupt handlers gives better responsiveness
for interactive applications and keeps I/O devices busy.
  However, if we delegate the updating of the bsd variables to happen outside
of the interrupt handler, we will leave the ready list in an inconsistent state
(some threads will not have their priority updated based on up to date BSD
variables).

---- RATIONALE ----

>> B5: (5 marks)
>> Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.

Our implementation takes some steps to avoid doing unnecessary work.
  We only recalculate the priority for threads that are ready (rather than all
threads, as having fresh priority values for blocked threads is not necessary,
instead we recalculate values for blocked threads when they become unblocked.)
  Our use of macros to do fixed point arithmetic also eliminates a lot of
runtime overhead compared to if we used function calls.
  We also kept a number of threads that are ready to run (including the current
running thread, if it is not the idle thread). This is to avoid counting the
number of ready threads when we update load_avg in the interrupt handler.
  However, our implementation does a lot of the computation inside of an
interrupt context, which is a disadvantage for the reasons listed above.
  We considered implementing a lock on the ready list to get around this,
however we cannot acquire a lock from an interrupt context in thread_tick, and
if we updated the BSD variables in a separate thread which acquired the lock
outside of the interrupt, it may not run for a while and the BSD varaibles will
have outdated values.

>> B6: (5 marks)
>> The assignment explains arithmetic for fixed-point mathematics in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point mathematics, that is, an abstract
>> data type and/or a set of functions or macros to manipulate
>> fixed-point numbers, why did you do so?  If not, why not?S

For the fixed-point arithmetic we chose to create a new file in the threads
directory called fixed-point.h. In this file, we defined the 'fixed_point' type
to be an integer that represents a 17.14 fixed-point number, as well as the
fixed-point operations required by our scheduler.
  Due to the fact that we have to continuously update important fixed-point
values in the scheduler to calculate thread priorities, we chose to implement
these arithmetic operations in the form of macros rather than functions. Macros
are replaced by the pre-processor with the defined values, which meant there
was no unnecessary overhead at runtime from repeated calls.
